---
title: "II. Design of experiments: Behaviour of e-variables under alternatives"
author: "Alexander Ly, Rosanne Turner, Udo Boehm, Rianne de Heide, Peter Grunwald"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{II. Design of experiments: Behaviour of e-variables under alternatives}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 4,
  fig.width = 8
)
```

In the previous markdown document we used simulations to illustrate the behaviour of three sequential testing procedures. We noticed that if data are generated under the null that the sequential use of both classical p-values and subjective Bayes factors leads to the incorrect conclusion that the null is false with chance larger than the tolerable alpha. On the other hand, with e-variables we retained type I error control regardless of the sample size. 

To compute an e-value, we first need to create a design object with a test defining parameter, which for the problem at hand is referred to as g. In this document we show how this parameter g can be set, for instance, by providing the design function with a minimal clinically relevant effect size. Hence, the value of the parameter can be derived under the alternative. We will see that the value of this parameter g, however, is not crucial. To illustrate this, we will elaborate on the following: 

1. In the [first section](#designZ) we elaborate on the safe design function, which determines the planned sample size when it is provided with a targeted power/tolerable type II error, and a minimal clinically relevant effect size. 
2. We show how the use of e-variables can lead to an increase in efficiency due to early stopping, whenever [the true effect size equals the minimal clinical relevant mean effect size](#effectSizeEqualMinimalRelevant).
3. Simulations with [a true effect size larger than the minimal clinically relevant effect size](#effectSizeLargerMinimalRelevant), then shows how even more efficiency can be gained. 
4. To further illustrate that a minimal clinically relevant effect size is not crucial for the results, we simulate under [a true effect size smaller than the minimal clinically relevant effect size](#effectSizeSmallerMinimalRelevant). 

<!-- if the true effect size equals the minimal clinical one,  -->
<!-- - We then  -->

<!-- if information regarding a minimal clinically relevant mean difference is available, we can  -->


<!-- We will see that the resulting e-values -->

<!-- We also elaborate on the design of experiments based on e-variables. The main parameter of such a design depends on a minimally clinically relevant mean difference. We will that this dependence is rather weak, as  -->

<!-- see that it is not crucial to get this right  -->

<!-- The main parameter to set for the  -->

<!-- , in particular, the choice of the parameter g, which is a function of the minimal clinically relevant mean difference. The conclusion will be that the choice of g is  -->

<!-- simulate under the alternative to study the behaviour  -->



<!-- : Based on (1) p-values, (2) subjective Bayes factors, and (3) e-variables. We saw that all three procedures allow for early stopping, but that for the first two procedures this comes with the high cost of drawing the wrong conclusion that the null is false, whenever it actually holds true.  -->

<!-- We also saw that if the null holds true, e-variables yield e-values below 1/alpha with less than alpha chance, regardless of the sample size. To save resources, the e-variable should yield e-values greater than 1/alpha, whenever the null is false. We will see that the e-variables typically follow this pattern, thus, accumulate evidence against the null quickly whenever it is false.  -->

<!-- In other words, we illustrate the behaviour of e-values under the alternative. The topics we cover are the following: -->

<!-- - The design of experiments based on e-variables -->
<!-- - How early stopping can be achieved if there truly is an effect -->

<!-- We again make use of the running example of the two-sample z-test discussed in the previous document.  -->

For this demonstration we use functions from the safestats package and the following colours:

```{r, eval=TRUE}
library("safestats")
eColours <- c("#1F78B4E6", "#A6CEE380")
lineColour <- "#DAA52066"
histInnerColour <- eColours[2]
histBorderColour <- eColours[1]
```

# 1. <a name = "designZ">Designing safe z-test experiments</a>
## One sample path
Recall the example described in the installation guide with a minimal clinically relevant mean difference of 10 (IQ score increase) in a population with standard deviation sigmaTrue=8. 

To detect such a minimal clinically relevant mean difference with 80\% chance/power, thus, tolerable type II error beta=0.2, we have to __plan__ an experiment with 18 secondary school students in both the treatment and the control group: 

```{r, eval=TRUE}
alpha <- 0.05
beta <- 0.2
meanDiffMin <- 10
sigmaTrue <- 8

designObj <- designSafeZ(meanDiffMin=meanDiffMin, beta=beta,
                         testType="twoSample",
                         sigma=sigmaTrue, pb=FALSE)
designObj
```
To compute a z-test e-value the function safeZTest uses the parameter defined in the designObj, which for the e-value type "eGauss" is referred to as g. This g is the variance of a so-called Gaussian mixture distribution, similar, to a prior in a Bayes factor. The design function sets g equal to meanDiffMin^2/sigma^2, that is `10^2/8^2=1.5625`. 

In the installation guide we noticed that for a particular sample generated with a true mean difference of meanDiffMin, we correctly reject the null, if we compute the e-value after observing the planned sample sizes.

```{r, eval=TRUE}
set.seed(1)
treatmentGroup <- rnorm(20, mean=122, sd=sigmaTrue)
controlGroup <- rnorm(20, mean=112, sd=sigmaTrue)

resultObj <- safeZTest(x=treatmentGroup, y=controlGroup,
                       designObj=designObj)
resultObj
```
The resulting e-value of 4600 is much larger than 1/alpha, and one might wonder whether we could have reject the null earlier. The following code shows that this is indeed the case, by monitoring the e-value after an observation of each group.

```{r, eval=TRUE}
n1Plan <- designObj$nPlan[1]
eValuesAtTime <- numeric(n1Plan)

for (i in 1:n1Plan) {
  eValuesAtTime[i] <- safeZTest(x=treatmentGroup[1:i],
                               y=controlGroup[1:i],
                               designObj=designObj)$eValue
}

oldPar <- setSafeStatsPlotOptionsAndReturnOldOnes()
plot(1:n1Plan, eValuesAtTime, type="l", lwd=2, xlab="n",
     ylab="e-value", col=eColours[1])
lines(c(1, n1Plan), c(1/alpha, 1/alpha), lwd=2, lty=2)
which(eValuesAtTime > 1/alpha)
```
The plot shows that we could have stopped after 10 pairs of observations rather than wait until we saw 18 pairs. Hence, this emphasises how the realised sample size is random, as it depends via the e-value on the data. The more compelling the evidence indicated by the e-value, the earlier we can stop sampling. This thus allows us to adapt the difficulty of the problem. 

# <a name="effectSizeEqualMinimalRelevant">Early stopping: True effect size equals the minimal clinical relevant mean effect size</a>
The planned sample sizes as reported by the design object is a result of a simulation under the alternative with true mean difference equal to the minimal clinically relevant effect size. More specifically, the "designSafeZ" function first determines the sample size(s) necessary to detect the effect with 80\% power if data analysis can only be performed once. In this case that is n1PlanBatch=n2PlanBatch=20. As multiple looks yield more opportunities to reject the null, this batch sample size is taken as an upper bound for the sample sizes in the simulation. The simulation is performed by the function "sampleStoppingTimesSafeZ", which by default generate a 1000 data sets of length nPlanBatch, and calculates the evalues as the data come in. The first-time at which the e-value passes the threshold of 1/alpha is then saved. The following code visualises the stopping times. 

```{r, eval=TRUE}
stoppingTimes <- designObj$bootObjN1Plan$data

oldPar <- setSafeStatsPlotOptionsAndReturnOldOnes()
hist(stoppingTimes, 
     breaks=min(stoppingTimes):max(stoppingTimes), 
     xlim=c(0, designObj$nPlanBatch[1]),col=histInnerColour, 
     border=histBorderColour, lwd=2, main="", density=100)
```
Note the bump at nPlanBatch, which contains both experiments that yielded e-values above and below 20. The following code shows only the first-passage times, the times at which the e-value crosses the threshold of 1/alpha, and some sample paths. 

```{r, eval=TRUE}
firstPassageTimes <-
  designObj$bootObjN1Plan$data[!designObj$breakVector]
firstPassageTimeHist <-
  hist(firstPassageTimes, plot=FALSE,
       breaks=min(firstPassageTimes):max(firstPassageTimes))

oldPar <- setSafeStatsPlotOptionsAndReturnOldOnes()
plot(NULL, xlim = c(0, designObj$nPlanBatch[1]),
     ylim = c(-1 * log(30), log(3000)),
     xlab = "", ylab = "", cex.lab = 1.3, cex.axis = 1.3, las = 1,
     yaxt = "n", bty = "n", type = "p", pch = 21, bg = "grey")

abline(h = log(1), col = "darkgrey", lwd = 1, lty = 2)
abline(h = log(20))

criticalP = log(c(1/20, 1, 20))

axis(side = 2, at = c(criticalP), tick = TRUE, las = 2, cex.axis = 1,
     labels = c("1/20", "1", "20"))

mtext("Evidence", side = 2, line = 2.5, las = 0, cex = 1.3, adj=0.175)
mtext("Sample size", side = 1, line = 2.5, las = 1, cex = 1.3)

stoppedPaths <- designObj$samplePaths[!designObj$breakVector, ]

someConstant <- 50

y <- firstPassageTimeHist$density
nB <- length(firstPassageTimeHist$breaks)
ylim <- range(y, 0)

rect(firstPassageTimeHist$breaks[-nB]+0.5, log(20),
     firstPassageTimeHist$breaks[-1L]+0.5, someConstant*y+log(20),
     col = histInnerColour, border = histBorderColour, lwd=2,
     angle = 45, density = 600, lty = NULL)

for (i in 1:100) {
  stoppedTime <- firstPassageTimes[i]
  evidenceLine <- stoppedPaths[i, 1:stoppedTime]

  if (evidenceLine[stoppedTime] > 20)
    evidenceLine[stoppedTime] <- 20

  lines(0:stoppedTime, c(0, log(evidenceLine)), col=lineColour,
        lwd=1.5, lty=1)

  if (evidenceLine[stoppedTime]==20)
    points(stoppedTime, log(evidenceLine[stoppedTime]),
           col=histBorderColour, pch=15, lwd=1.5)
}
```

The 80th percentile of the plotted first passage time distribution is used as the planned sample size in the designObj. 


# <a name="effectSizeLargerMinimalRelevant">Even earlier stopping: True effect size larger than the minimal clinical relevant mean effect size</a>


# <a name="effectSizeSmallerMinimalRelevant">Optional continuation: True effect size smaller than the minimal clinical relevant mean effect size</a>

# Concluding remarks
